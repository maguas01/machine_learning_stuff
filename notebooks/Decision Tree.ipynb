{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "![dtree](img/d_tree.png)\n",
    "\n",
    "- we can create a decision tree from variables. \n",
    "    - In this example we wish to create a decision tree from occupation and genter such that we can try to choose the best recomended app. \n",
    "    - a good strategy is to choose the variable which has the least edges to end . \n",
    "    - here we choose the node occupation as school imidiatly recomends an app pokenmon go\n",
    "    - occupation also leads to work node. The work node leads to the gender variable. \n",
    "    - the gender variable leads to female male node which will help us choose a reconded app. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can do the same with continous variables \n",
    "\n",
    "![d_tree_cont](img/d_tree_cont.png)\n",
    "\n",
    "- in this example we have grades vs test. Blue dots represent those who are admited. Red dots are those who were not admited\n",
    "- this graph can be seperated by a vertical line where most of those who were admited on the right (x >= 5 ) and most of those who where on the left were not admited ( x < 5 )\n",
    "    - with this information we can begin to build a decision tree again we chose the variable which splits out data best in this case test. so our first node is test\n",
    "    - from test we have two node if test is >=5 then we can look at thier grades  if grades are >= 2 admit else reject\n",
    "    - the second node in test is if test < 5 if we then look at the next node its grades the line that splits the test best is 7 so if grades is <= 7 admit else reject \n",
    "\n",
    "- entropy: one way to look at it is homogenaity, a set that is homogenous has low entropy and high know alternatively a set that has not homogenous with have higher entropy and low knowlage, this is not an exact def but a good way to think about entropy. \n",
    "\n",
    "- we can use the log(x) to to change from products of number to sum of number that is $\\log_{2}{ab}=\\log_{2}{a} + \\log_{2}{b}$\n",
    "\n",
    "\n",
    "- $entropy = -p_1\\log_2{\\left(\\frac{p_1}{p_1 + p_2 + \\dots + p_n}\\right)}-\\dots -p_n\\log_2{\\left(\\frac{p_n}{p_1+p_2+ \\dots +p_n} \\right)}=$\n",
    "$-\\sum_i^np_i\\log_2{\\left(\\frac{p_i}{\\sum_i^np_i} \\right)}$\n",
    "\n",
    "suppose we had : If we have a bucket with eight red balls, three blue balls, and two yellow balls, what is the entropy of the set of balls? Input your answer to at least three decimal places. we can calculate this using the formula above \n",
    "\n",
    "https://www.wolframalpha.com/input/?i=-(8%2F13)log_2(8%2F13)-(3%2F13)log_2(3%2F13)-(2%2F13)log_2(2%2F13)\n",
    "1.33467914105159458006133443642258323160694076924661866327944368248721713406509640110377334376159607120085342493875...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

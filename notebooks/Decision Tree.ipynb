{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "![dtree](img/d_tree.png)\n",
    "\n",
    "- we can create a decision tree from variables. \n",
    "    - In this example we wish to create a decision tree from occupation and genter such that we can try to choose the best recomended app. \n",
    "    - a good strategy is to choose the variable which has the least edges to end . \n",
    "    - here we choose the node occupation as school imidiatly recomends an app pokenmon go\n",
    "    - occupation also leads to work node. The work node leads to the gender variable. \n",
    "    - the gender variable leads to female male node which will help us choose a reconded app. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can do the same with continous variables \n",
    "\n",
    "![d_tree_cont](img/d_tree_cont.png)\n",
    "\n",
    "- in this example we have grades vs test. Blue dots represent those who are admited. Red dots are those who were not admited\n",
    "- this graph can be seperated by a vertical line where most of those who were admited on the right (x >= 5 ) and most of those who where on the left were not admited ( x < 5 )\n",
    "    - with this information we can begin to build a decision tree again we chose the variable which splits out data best in this case test. so our first node is test\n",
    "    - from test we have two node if test is >=5 then we can look at thier grades  if grades are >= 2 admit else reject\n",
    "    - the second node in test is if test < 5 if we then look at the next node its grades the line that splits the test best is 7 so if grades is <= 7 admit else reject \n",
    "\n",
    "- entropy: one way to look at it is homogenaity, a set that is homogenous has low entropy and high know alternatively a set that has not homogenous with have higher entropy and low knowlage, this is not an exact def but a good way to think about entropy. \n",
    "\n",
    "- we can use the log(x) to to change from products of number to sum of number that is $\\log_{2}{ab}=\\log_{2}{a} + \\log_{2}{b}$\n",
    "\n",
    "\n",
    "$entropy=-\\left(\\frac{p_1}{p_1 + p_2 + \\dots + p_n}\\right)\\log_2{\\left(\\frac{p_1}{p_1 + p_2 + \\dots + p_n}\\right)}-\\dots-\\left(\\frac{p_n}{p_1 + p_2 + \\dots + p_n}\\right)\\log_2{\\left(\\frac{p_n}{p_1+p_2+ \\dots +p_n} \\right)}$\n",
    "\n",
    "$entropy=-\\sum_i^n\\left(\\frac{p_i}{\\sum_j^np_j} \\right)\\log_2{\\left(\\frac{p_i}{\\sum_j^np_j} \\right)}$\n",
    "\n",
    "$entropy=-\\sum_i^nprob_i\\log_2{(prob_i)}$\n",
    "\n",
    "If we have a bucket with eight red balls, three blue balls, and two yellow balls, what is the entropy of the set of balls? Input your answer to at least three decimal places. we can calculate this using the formula above \n",
    "\n",
    "https://www.wolframalpha.com/input/?i=-(8%2F13)log_2(8%2F13)-(3%2F13)log_2(3%2F13)-(2%2F13)log_2(2%2F13)\n",
    "1.33467914105159458006133443642258323160694076924661866327944368248721713406509640110377334376159607120085342493875...\n",
    "$information\\_gain=Entropy(parent) - (\\frac{m}{m+n}Entropy(child1) + \\frac{n}{m+n}Entropy(child2))$ \n",
    "\n",
    "where m and n are the size of the group. \n",
    "\n",
    "- we can build the recomneded apps decision tree by choosing variables which net the highest information gain. \n",
    "    - first calculate entropy of the parent (apps) \n",
    "    \n",
    "    $entropy\\_parent=-\\frac{3}{6}\\log_2{(\\frac{3}{6})}-\\frac{2}{6}\\log_2{(\\frac{2}{6})}-\\frac{1}{6}\\log_2{(\\frac{1}{6})}$\n",
    "    \n",
    "    $entropy\\_parent = 1.46$\n",
    "- now we calculate the information gain for both gender and occupation and choose the variable with the highest information gain\n",
    "\n",
    "gender: \n",
    "\n",
    "$entropy\\_male=-\\frac{1}{3}\\log_2{\\left(\\frac{1}{3}\\right)}-\\frac{2}{3}\\log_2{\\left(\\frac{2}{3}\\right)}$\n",
    "\n",
    "$entropy\\_male= 0.92$\n",
    "\n",
    "$entropy\\_female=-\\frac{1}{3}\\log_2{\\left(\\frac{1}{3}\\right)}-\\frac{2}{3}\\log_2{\\left(\\frac{2}{3}\\right)}$\n",
    "\n",
    "$entropy\\_female= 0.92$\n",
    "\n",
    "$information\\_gain=1.46 -\\left(\\frac{3}{6}0.92 + \\frac{3}{6}0.92\\right)$\n",
    "\n",
    "$information\\_gain=0.54$\n",
    "\n",
    "occupation: \n",
    "\n",
    "$entropy\\_study=-\\frac{3}{3}\\log_2{\\left(\\frac{3}{3}\\right)}$\n",
    "\n",
    "$entropy\\_study=0$\n",
    "\n",
    "$entropy\\_work=-\\frac{1}{3}\\log_2{\\left(\\frac{1}{3}\\right)}-\\frac{2}{3}\\log_2{\\left(\\frac{2}{3}\\right)}$\n",
    "\n",
    "$entropy\\_work=0.92$\n",
    "\n",
    "$information\\_gain=1.46-\\left(\\frac{3}{6}0 + \\frac{3}{6}0.92\\right)$\n",
    "\n",
    "$information\\_gain=1$\n",
    "\n",
    "- hence we should choose occupation as the first parent child nodes. we can repeat this proccess to complete the tree. \n",
    "- one problem with decision trees is overfitting. We can fix this problem by using a random forests. We can randomly choose a few variable and create several trees. \n",
    "\n",
    "![random-for](img/random_forest.png)\n",
    "\n",
    "- we let all the tree make a decision and simply choose the decision which is made the most by the random forest. \n",
    "\n",
    "Hyperparameters: different aspects of a decision tree\n",
    "\n",
    "    maximum depth of tree\n",
    "    minimum number of samples per leaf\n",
    "    minimum number of samples per split\n",
    "    maximum number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
